# Agent Architecture Comparison: ReAct vs Structured Flows

An experimental comparison of two fundamental AI agent paradigms applied to e-commerce sales scenarios.

## ğŸ¯ Overview

This project explores the practical differences between **ReAct agents** (reasoning + acting) and **Structured Workflow agents** (StateGraph-based) in handling customer sales interactions.

**Key Question**: Which architecture performs better for real-world business applications?

## âš ï¸ Important Disclaimers

**This is an exploratory experiment, not rigorous research:**

- âŒ **Small sample size** (n=15 test cases)
- âŒ **Single domain** (e-commerce only)
- âŒ **Subjective metrics** (LLM-evaluated)
- âŒ **Limited statistical significance**

**Intended as:**
- âœ… Learning exercise shared publicly
- âœ… Architecture decision exploration
- âœ… Starting point for community discussion
- âœ… Practical comparison methodology

## ğŸ—ï¸ Architecture Comparison

### ReAct Agent
- **Framework**: LangGraph's `create_react_agent`
- **Approach**: Dynamic reasoning and tool selection
- **Strengths**: Flexibility, handles complex scenarios
- **Weaknesses**: Unpredictable, potentially slower

### Structured Flow Agent  
- **Framework**: StateGraph with defined states
- **Approach**: Fixed pipeline with clear steps
- **Strengths**: Predictable, consistent output
- **Weaknesses**: Less adaptable to edge cases

## ğŸ§ª Experimental Setup

### Test Environment
- **Products**: 27 items across 6 categories
- **Test Cases**: 15 scenarios (basic, ambiguous, multi-product, comparison)
- **Evaluation**: GPT-4 as judge across 5 dimensions
- **Metrics**: Extraction, Information, Process, Communication, Resolution

### Tools Available to Both Agents
- `buscar_producto()` - Product information lookup
- `verificar_stock()` - Stock availability check
- `calcular_precio_total()` - Price calculation with discounts
- `procesar_pedido()` - Order processing

## ğŸ“Š Sample Results

*Results vary by test case complexity and type*

| Metric | ReAct | Structured Flow |
|--------|-------|-----------------|
| Success Rate | Variable | Consistent |
| Avg Response Time | Higher variance | Lower variance |
| Complex Cases | Better handling | More rigid |
| Simple Cases | Potential overkill | Efficient |

## ğŸš€ Quick Start

```bash
# Clone the repository
git clone https://github.com/yourusername/agent-architecture-comparison.git
cd agent-architecture-comparison

# Install dependencies
pip install -r requirements.txt

# Set up environment variables
cp .env.example .env
# Add your OPENAI_API_KEY to .env

# Run the experiment
python main.py
```

## ğŸ“ Project Structure

```
â”œâ”€â”€ main.py                 # Main experiment runner
â”œâ”€â”€ inventario_completo.json # Product catalog
â”œâ”€â”€ requirements.txt        # Dependencies
â”œâ”€â”€ .env.example           # Environment template
â””â”€â”€ README.md              # This file
```

## ğŸ¤ Contributing

This experiment has many opportunities for improvement:

### Easy Improvements
- [ ] Increase test case variety
- [ ] Add more product categories
- [ ] Implement human evaluation baseline
- [ ] Add cost analysis (API calls)

### Advanced Improvements  
- [ ] Statistical significance testing
- [ ] Multi-domain evaluation
- [ ] Latency distribution analysis
- [ ] A/B testing framework

**Pull requests welcome!** Please read our contribution guidelines.

## ğŸ’¡ Key Insights

*From this limited experiment:*

1. **Context matters more than architecture** - The specific use case drives which approach works better
2. **ReAct excels at complex, ambiguous scenarios** - Better flexibility for edge cases
3. **Structured flows win on consistency** - More predictable behavior and timing
4. **Hybrid approaches may be optimal** - Route simple cases to flows, complex to ReAct

## ğŸ”¬ Methodology Limitations

**Statistical Issues:**
- Small sample size limits generalizability
- No confidence intervals or significance testing
- Single evaluator (GPT-4) introduces bias

**Experimental Issues:**
- Toy dataset vs real-world complexity
- Fixed timeout differences between approaches
- No multi-run averaging for stability

**Scope Issues:**
- Single domain (e-commerce) limits applicability
- English-only test cases
- No production environment testing

## ğŸ“š Related Work

- [ReAct: Synergizing Reasoning and Acting in Language Models](https://arxiv.org/abs/2210.03629)
- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)
- [Agent Architecture Patterns](https://blog.langchain.dev/planning-for-agents/)

## ğŸ“„ License

MIT License - Feel free to use, modify, and share.

## ğŸ™‹â€â™€ï¸ About

Created by Aina-lluna as a learning exercise in AI agent architectures. 

**Background**: AI Generative Lead exploring practical implications of different agent paradigms for business applications.

**Contact**: [[mi LinkedIn]](https://www.linkedin.com/in/ainataylor/) 

---

*Made with curiosity, shared for learning* ğŸ§ 

**â­ If this helps your understanding of agent architectures, please star the repo!**
